---
title: "Stat 618 Project"
author: "Aaron Niecestro"
date: "December 5, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
lapply(c("rjags","arm","coda","superdiag","R2WinBUGS","R2jags","lme4"), library,
       character.only=TRUE)
```

## Data

```{r Data tidying}
DC_Properties <- read.csv("~/Documents/STAT 627 Statistical Machine Learning/Stat 627 Project/Data/DC_Properties.csv")
DC_Properties <- data.frame(DC_Properties) # list to data.frame fix

## Final Cleaned Dataset

DC_Properties[is.na(DC_Properties)] <- 0 # setting all NA to 0 
# makes data easier to clean and work with

## Final Cleaned Dataset

select <- dplyr::select

DC_Properties_tidy <- DC_Properties %>%
  select(PRICE, BATHRM, HF_BATHRM, HEAT, AC, ROOMS, BEDRM, AYB, YR_RMDL, EYB, STORIES, QUALIFIED, GRADE, CNDTN, KITCHENS, FIREPLACES, WARD, QUADRANT, LATITUDE, LONGITUDE) %>%
  mutate(PRICE = as.numeric(PRICE),
         BATHRM = as.numeric(BATHRM),
         HF_BATHRM = as.numeric(HF_BATHRM),
         ROOMS = as.numeric(ROOMS),
         BEDRM = as.numeric(BEDRM),
         AYB = as.numeric(AYB),
         EYB = as.numeric(EYB),
         STORIES = as.numeric(STORIES),
         KITCHENS = as.numeric(KITCHENS),
         FIREPLACES = as.numeric(FIREPLACES),
         LATITUDE = as.numeric(LATITUDE),
         LONGITUDE = as.numeric(LONGITUDE),
         HEAT = as.character(HEAT), # made character columns here to filter and clean data better
         AC = as.character(AC),
         QUALIFIED = as.character(QUALIFIED),
         GRADE = as.character(GRADE),
         CNDTN = as.character(CNDTN)) %>%
  filter(CNDTN != "",
         CNDTN != "Default",
         CNDTN != "Poor",
         GRADE != " No Data",
         GRADE != "",
         HEAT != "No Data", # lose ~50,000 observations of the data by here
         PRICE > 10000 & PRICE < 10000000,
         FIREPLACES < 10,
         KITCHENS <= 10,
         ROOMS <= 40,
         BEDRM <= 20,
         STORIES <= 10,
         LATITUDE != 0,
         LONGITUDE != 0) %>% # Up to here, I lose roughly 100K observations
  mutate(AYB.age = AYB, # making new quantitative
         AYB.age = 2019 - AYB.age,
         AYB.age = ifelse(AYB == 2019, 0, AYB.age),
         EYB.age = as.numeric(EYB),
         EYB.age = 2019 - EYB.age,
         EYB.age = ifelse(EYB == 2019, 0, EYB.age),
         REMODEL.age = as.character(YR_RMDL),
         REMODEL.age = ifelse(REMODEL.age == "0", 0, REMODEL.age),
         REMODEL.age = ifelse(REMODEL.age == "20", 0, REMODEL.age),
         REMODEL.age = as.numeric(REMODEL.age),
         REMODEL.age = 2019 - REMODEL.age,
         REMODEL.age = ifelse(REMODEL.age == 2019, 0, REMODEL.age),
         GRADE = ifelse(GRADE == "Exceptional-A", "Exceptional", GRADE), # fixing Grade
         GRADE = ifelse(GRADE == "Exceptional-B", "Exceptional", GRADE),
         GRADE = ifelse(GRADE == "Exceptional-C", "Exceptional", GRADE),
         GRADE = ifelse(GRADE == "Exceptional-D", "Exceptional", GRADE),
         QUALIFIED_2 = QUALIFIED, # making new Qualified Variable
         QUALIFIED_2 = ifelse(QUALIFIED == "Q", 2, 1),
         QUALIFIED_2 = as.factor(QUALIFIED_2),
         AC = ifelse(AC == "0", "N", AC),
         GRADE = as.factor(GRADE), # fixed certain variables back to factors
         HEAT = as.factor(HEAT), 
         AC = as.factor(AC),
         WARD = as.factor(WARD),
         CONDITION = as.factor(CNDTN),
         QUALIFIED = as.factor(QUALIFIED),
         AYB.age = as.numeric(AYB.age),
         EYB.age = as.numeric(EYB.age),
         REMODEL.age = as.numeric(REMODEL.age))%>%
  select(-CNDTN, -AYB, -YR_RMDL, -EYB) # select again 

DC_Final <- na.omit(DC_Properties_tidy)

DC_Final <- DC_Final %>%
  filter(AYB.age < 2000) %>%
  mutate(PRICE_10K = PRICE/10000,
         BATHRM = BATHRM + HF_BATHRM*.5,
         BATHRM = as.numeric(BATHRM),
         GRADE = as.integer(GRADE),
         WARD = as.integer(WARD),
         CONDITION = as.integer(CONDITION),
         HEAT = as.integer(HEAT)) %>%
  select(PRICE, PRICE_10K, BATHRM, ROOMS, BEDRM, STORIES, KITCHENS, FIREPLACES, LATITUDE,
         LONGITUDE, AYB.age, EYB.age, REMODEL.age, HEAT, AC, QUALIFIED, QUALIFIED_2, GRADE,
         WARD, QUADRANT, CONDITION)

set.seed(10000000)

DC_Final <- sample_n(DC_Final, 57610, replace = TRUE) # want the datasets to be even 
# I lost one observation by doing this
dim(DC_Final)
summary(DC_Final)

## random case number

n <- length(DC_Final$PRICE)
Z <- sample(n,n/2)

bayes.train <- DC_Final[Z,] %>%
  as.list()
bayes.test <- DC_Final[-Z,] %>%
  as.list()

# kitchens needs NA switched to 0
# stories has a lot of NAs = 52305, mutate NA or "" to 0
# Price also has a lot of NA = 60741, mutate NA to 0
```

### Final Model

```{r}

```

### Diagnostics

```{r}
# Data Diagnostics

# Basic Model Diagnostics
plot(model.grade)
plot(asap.mcmc1)
traceplot(model.grade, mfrow = c(3,3),
          varname = c("beta", "tau.y", "tau.alpha"),
          ask = FALSE)
```



### Models

```{r}
## Model with Grade Grouping

# PRICE ~ BATHRM + ROOMS + BEDRM + STORIES + QUALIFIED + KITCHENS + FIREPLACES + AYB.age + EYB.age + REMODEL.age

# DEFINE THE MODEL
model.group.grade  <- function()  { # model name
    for (i in 1:N) { # 1st for loop
      mu[i] <- alpha[GRADE[i]] + beta[1]*BATHRM[i] + beta[2]*ROOMS[i] + beta[3]*BEDRM[i] +
        beta[4]*KITCHENS[i] + beta[5]*FIREPLACES[i] + beta[6]*AYB.age[i] + beta[7]*EYB.age[i] +
        beta[8]*REMODEL.age[i] + beta[9]*REMODEL.age[i]*CONDITION[i]
      PRICE_10K[i] ~ dnorm(mu[i],tau.y) # response variable distribution
      e.y[i] <- PRICE_10K[i] - mu[i]
    }
    beta[1]   ~ dnorm(0,0.0001); # beta distribution
    beta[2]   ~ dnorm(0,0.0001);
    beta[3]   ~ dnorm(0,0.0001);
    beta[4]   ~ dnorm(0,0.0001);
    beta[5]   ~ dnorm(0,0.0001);
    beta[6]   ~ dnorm(0,0.0001);
    beta[7]   ~ dnorm(0,0.0001);
    beta[8]   ~ dnorm(0,0.0001);
    beta[9]   ~ dnorm(0,0.0001);
    tau.y   ~ dgamma(1,0.1); # tau distributions
    
    s.y <- sd(e.y[])
    
    for (j in 1:J) { # 2nd for loop
      alpha[j] ~ dnorm(0,tau.alpha) # grouping variable for newpid distribution
    }
    tau.alpha ~ dgamma(1,0.1);
}

train_jags <- bayes.train # changed dataset names so I know this dataset is for jags
train_jags["N"] <- length(bayes.train[[1]]) # 1st for loop end number
train_jags["J"] <- length(unique(bayes.train$GRADE)) # 2nd for loop end number

# SETUP INITIAL VALUES AND PARAMETER NAMES
grade.inits <- function(){ 
  list("tau.y" = 1, "tau.alpha" = 1, "beta" = rep(1,9)) 
}
grade.params <- c("beta", "tau.y", "tau.alpha", "s.y")

# RUN THE SAMPLER AND COLLECT coda SAMPLES
model.grade <- jags(data=train_jags, inits = grade.inits, grade.params, n.iter=1000, model=model.group.grade, DIC = TRUE)
print(model.grade)
```

```{r}
asap.out1 <- update(model.grade, n.iter=2000)
print(asap.out1)
asap.mcmc1 <- as.mcmc(asap.out1)
superdiag(as.mcmc.list(asap.mcmc1), burnin=0)
```

```{r}
plot(model.grade)
plot(asap.mcmc1)
traceplot(model.grade, mfrow = c(3,3),
          varname = c("beta", "tau.y", "tau.alpha"),
          ask = FALSE)
```

### Attempted Models

```{r}
## Model with Condition Grouping

# PRICE ~ BATHRM + ROOMS + BEDRM + STORIES + QUALIFIED + KITCHENS + FIREPLACES + AYB.age + EYB.age + REMODEL.age

# DEFINE THE MODEL
model.group.condition <- function()  { # model name
    for (i in 1:N) { # 1st for loop
      mu[i] <- alpha[CONDITION[i]] + beta[1]*BATHRM[i] + beta[2]*ROOMS[i] + beta[3]*BEDRM[i] +beta[4]*KITCHENS[i] + beta[5]*FIREPLACES[i] + beta[6]*AYB.age[i] + beta[7]*EYB.age[i] + beta[8]*REMODEL.age[i]
      # model with3 betas and newpid grouping variables
      PRICE_10K[i] ~ dnorm(mu[i],tau.y) # response variable distribution
    }
    beta[2]   ~ dnorm(0,0.0001);
    beta[3]   ~ dnorm(0,0.0001);
    beta[4]   ~ dnorm(0,0.0001);
    beta[5]   ~ dnorm(0,0.0001);
    beta[6]   ~ dnorm(0,0.0001);
    beta[7]   ~ dnorm(0,0.0001);
    beta[8]   ~ dnorm(0,0.0001);
    tau.y   ~ dgamma(1,0.1); # tau distributions
    tau.alpha ~ dgamma(1,0.1);
    
    for (j in 1:J) { # 2nd for loop
      other[j] ~ gamma[1]*GRADE[j] + gamma[2]*QUALIFIED[j]
      alpha[j] ~ dnorm(other[j], tau.alpha)
    }

}

train_jags <- bayes.train # changed dataset names so I know this dataset is for jags
train_jags["N"] <- length(bayes.train[[1]]) # 1st for loop end number
train_jags["J"] <- length(unique(bayes.train$CONDITION)) # 2nd for loop end number

# SETUP INITIAL VALUES AND PARAMETER NAMES
cond.inits <- function(){ 
  list("tau.y" = 1, "tau.alpha" = 1, "beta" = rep(1,8), "gamma" = c(1,1)) 
}
cond.params <- c("beta", "tau.y", "tau.alpha", "gamma")

# RUN THE SAMPLER AND COLLECT coda SAMPLES
asap.model.jags1 <- jags(data=train_jags, inits = cond.inits, cond.params, n.iter=1000, model=model.group.condition, DIC = TRUE)
print(asap.model.jags1)

asap.out1 <- update(asap.model.jags1, n.iter=2000)
print(asap.out1)
asap.mcmc1 <- as.mcmc(asap.out1)
superdiag(as.mcmc.list(asap.mcmc1), burnin=0)
```

```{r}
traceplot(asap.model.jags1, mfrow = c(3,3),
          varname = c("beta", "tau.y", "tau.alpha", "gamma"),
          ask = FALSE)
```

```{r}
## Model with Condition Grouping

# PRICE ~ BATHRM + ROOMS + BEDRM + STORIES + QUALIFIED + KITCHENS + FIREPLACES + AYB.age + EYB.age + REMODEL.age

# DEFINE THE MODEL
model.group.condition <- function()  { # model name
    for (i in 1:N) { # 1st for loop
      mu[i] <- alpha[CONDITION[i]] + beta[1]*BATHRM[i] + beta[2]*ROOMS[i] + beta[3]*BEDRM[i] + beta[4]*KITCHENS[i] + beta[5]*FIREPLACES[i] + beta[6]*AYB.age[i] + beta[7]*EYB.age[i] + beta[8]*REMODEL.age[i]
      # model with3 betas and newpid grouping variables
      PRICE_10K[i] ~ dnorm(mu[i],tau.y) # response variable distribution
    }
    beta[1]   ~ dnorm(0,0.0001);
    beta[2]   ~ dnorm(0,0.0001);
    beta[3]   ~ dnorm(0,0.0001);
    beta[4]   ~ dnorm(0,0.0001);
    beta[5]   ~ dnorm(0,0.0001);
    beta[6]   ~ dnorm(0,0.0001);
    beta[7]   ~ dnorm(0,0.0001);
    beta[8]   ~ dnorm(0,0.0001);
    beta[9]   ~ dnorm(0,0.0001);
    tau.y   ~ dgamma(1,0.1); # tau distributions
    tau.alpha ~ dgamma(1,0.1);
    
    for (j in 1:J) { # 2nd for loop
      alpha[j] ~ dnorm(other[j], tau.alpha)
    }

}

train_jags <- bayes.train # changed dataset names so I know this dataset is for jags
train_jags["N"] <- length(bayes.train[[1]]) # 1st for loop end number
train_jags["J"] <- length(unique(bayes.train$CONDITION)) # 2nd for loop end number

# SETUP INITIAL VALUES AND PARAMETER NAMES
grade.inits <- function(){ 
  list("tau.y" = 1, "tau.alpha" = 1, "beta" = rep(1,8)) 
}
grade.params <- c("beta", "tau.y", "tau.alpha")

# RUN THE SAMPLER AND COLLECT coda SAMPLES
asap.model.jags2 <- jags(data=train_jags, inits = grade.inits, grade.params, n.iter=1000, model=model.group.condition, DIC = TRUE)
print(asap.model.jags2)

asap.out1 <- update(asap.model.jags2, n.iter=2000)
print(asap.out2)
asap.mcmc1 <- as.mcmc(asap.out2)
superdiag(as.mcmc.list(asap.mcmc2), burnin=0)
```

```{r}
traceplot(asap.model.jags1, mfrow = c(3,3),
          varname = c("beta", "tau.y", "tau.alpha", "gamma"),
          ask = FALSE)
```

